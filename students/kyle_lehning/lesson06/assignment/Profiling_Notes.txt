1.) Running poor_perf results in printing 0 for '2018'. I knew that there should be some entries
with 2018 so I found the line that sets the counts and updated it to correctly add to the 2018 count
instead of the 2017 count.
2.) The csv file was opened and looped through twice. In good_perf I only opened the file once and
included the ao check during the same loop as the check for whether the date is more recent than
the beginning of 2012. With opening and running twice it took a total of 4.291531324s. In one loop
this was cut to 2.697790146s for an improvement of 1.613629864s.
3.) The for loop for figuring out which year the item was in used separate if statements and took
0.657239676s. Changing all the if statements to elif statements cut the time to 0.489697218s for an
improvement of 0.167542458s.
4.) Creation of a new dictionary with dates newer than 2012 then later processing those dates proved
unnecessary. Instead of making the new dictionary I updated the code to add to the date dictionary
as soon as it was found to be later than 2012. With updates 1-3 the code took 2.509312391s to run
analyze and with this additional update the code got down to 2.222040176s for an improvement of
0.287272215s
5.) While updating the year count to update the dictionary directly using the key without an if
statement it was found that the check for whether the date was newer than 2012 wasn't working. It
was also found that this should be greater than 2012. Updating this to work and removing the if
check entirely took total time from steps 1-4 of 2.222040176s  to run analyze, down to 2.086389780s
for an improvement of 0.135650396s.

Re-running poor_perf vs good_perf after all changes it was found that poor_perf took around 4.3s
to run analyze and good_perf now takes around 2.1s to run analyze for an improvement of 2.2s.
Running timeit on the main module iterating 10 times resulted in about 49s for poor_perf and about
23s for good_perf which is better than twice as fast.