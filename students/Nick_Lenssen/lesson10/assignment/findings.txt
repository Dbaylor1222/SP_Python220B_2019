I first altered the code from lesson 5 to add a timing feature that performed any interaction
with the MongDB connection. 

I ran through the larger files that were expanded by data_expander.py from lesson 7:

Function: import_data, Time: 0:00:01.665313, Records Processed: (1000, 1000, 10)
Function: show_available_products, Time: 0:00:00.020022, Records Processed: (0, 0, 0)
Function: show_rentals, Time: 0:00:00.007792, Records Processed: (0, 0, 0)
Function: clear, Time: 0:00:00.131609, Records Processed: (1000, 1000, 10)

Then ran the original smaller files from lesson 5:

Function: import_data, Time: 0:00:00.249475, Records Processed: (5, 10, 10)
Function: show_available_products, Time: 0:00:00.005994, Records Processed: (0, 0, 0)
Function: show_rentals, Time: 0:00:00.009021, Records Processed: (0, 0, 0)
Function: clear, Time: 0:00:00.155787, Records Processed: (5, 10, 10)

100:

Function: import_data, Time: 0:00:00.365351, Records Processed: (100, 100, 10)
Function: show_available_products, Time: 0:00:00.006205, Records Processed: (0, 0, 0)
Function: show_rentals, Time: 0:00:00.006412, Records Processed: (0, 0, 0)
Function: clear, Time: 0:00:00.111300, Records Processed: (100, 100, 10)


Ran very large files 10000:

Function: import_data, Time: 0:00:11.938967, Records Processed: (10000, 10000, 10)
Function: show_available_products, Time: 0:00:00.064599, Records Processed: (0, 0, 0)
Function: show_rentals, Time: 0:00:00.014858, Records Processed: (0, 0, 0)
Function: clear, Time: 0:00:00.112326, Records Processed: (10000, 10000, 10)


Simply creating a dictionary from all imputs has a low demand on processing time. Adding entries to 
the database takes significantly longer and will increase an order of magnitude if the data size
increases in an order of magnitude. The size of the data input should be taken into account
before going with this same approach