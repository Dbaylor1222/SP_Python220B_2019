Neima Schafi - Lesson06 Refactoring Notes
See (6) for empirical results.

(1) Following step/filtering for lrow[5] > 2012 unneeded. Why? Updated program to increment count for year using dict capabilities.

    REMOVED:
    for row in reader:
                lrow = list(row)
                if lrow[5] > '00/00/2012':
                    new_ones.append((lrow[5], lrow[0]))

(2) Updated code to only open csv once for manipulation, rather than 2 times - which was unnecessary.

(3) Refactored the way the code filters and increments years_count dict. See below.

    OLD:
    for new in new_ones:
                if new[0][6:] == '2013':
                    year_count["2013"] += 1
                if new[0][6:] == '2014':
                    year_count["2014"] += 1
                if new[0][6:] == '2015':
                    year_count["2015"] += 1
                if new[0][6:] == '2016':
                    year_count["2016"] += 1
                if new[0][6:] == '2017':
                    year_count["2017"] += 1
                if new[0][6:] == '2018':
                    year_count["2017"] += 1

    NEW:
    try:
      year_count[row[5]] += 1
    except KeyError:
      continue


  (4) No change of code for filtering for 'ao' so there shouldn't be any change in performance.
      Note: I changed found = 0 to found = [0] (a list rather than an int) in both sets of code so I could use timeit.

  (5) Comparison of results:

      poor_perf.py results/outputs:
      -----------------------------
      {'2013': 111347, '2014': 111010, '2015': 111373, '2016': 111869, '2017': 111135, '2018': 110574}
      'ao' was found 500016 times

      good_perf.py results/outputs:
      -----------------------------
      {'2013': 111347, '2014': 111010, '2015': 111373, '2016': 111869, '2017': 111135, '2018': 110574}
      'ao' was found 500016 times

      Validation that even though both sets of code are different, they produce the same results.


  (6) Raw data

      poor_perf.py run times:
      -----------------------
      INFO:__main__:Run time for row reader and filter for > 00/00/2012: 4.422955003999999 sec
      INFO:__main__:Sorting and counting matching years run time: 1.820586006 sec
      INFO:__main__:Run time for reading and filtering for "ao": 4.027639647999999 sec
      INFO:__main__:Program run time: 10.453176007 sec
      INFO:__main__:main() run time: 10.45275429 sec


      good_perf.py - run times:
      -------------------------
      INFO:__main__:Run time for counting years and filtering for "ao": 4.458554499 sec
      INFO:__main__:Program run time: 4.0768071290000005 sec - 38% faster
      INFO:__main__:main() run time: 3.9825367939999996 sec - 39% faster

      In conclusion, based on the results of run times above, the good_perf.py code was roughly 40% faster than the poor_perf.py code.
