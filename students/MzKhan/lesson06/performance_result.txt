1) Performance analysis using the OS time utility. Total number of rows are 1 million. 
$ time -p python poor_perf.py
{'2013': 129045, '2014': 129686, '2015': 129204, '2016': 129596, '2017': 129596,
 '2018': 129865}
'ao' was found 499039 times
real 8.17
user 0.01
sys 0.01

$ time -p python good_perf.py
{'2013': 129045, '2014': 129686, '2015': 129204, '2016': 129596, '2017': 129596,
 '2018': 129865}
'ao' was found 499039 times
real 3.37
user 0.01
sys 0.01

The real time is reduced significantly in the good_perf module. 

2) Use the cProfile library to analyze the performance. Total number of rows are 1 million. 

$ python -m cProfile -s cumtime poor_perf.py
{'2013': 129045, '2014': 129686, '2015': 129204, '2016': 129596, '2017': 129596,
 '2018': 129865}
'ao' was found 499039 times
         1039550 function calls (1039533 primitive calls) in 7.049 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    7.049    7.049 poor_perf.py:4(<module>)
        1    0.093    0.093    7.045    7.045 poor_perf.py:59(main)
        1    6.476    6.476    6.952    6.952 poor_perf.py:9(analyze)


$ python -m cProfile -s cumtime good_perf.py
{'2013': 129045, '2014': 129686, '2015': 129204, '2016': 129596, '2017': 129596,
 '2018': 129865}
'ao' was found 499039 times
         20363 function calls (20346 primitive calls) in 2.786 seconds

   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.000    0.000    2.786    2.786 good_perf.py:3(<module>)
        1    2.600    2.600    2.782    2.782 good_perf.py:8(analyze)

It is noticed that the performance improved by almost 2.7 times in the good_perf module. 
The improvement is very signifcant in this small piece of code. I can see the importance of implementing the right data structure 
to improve the efficiency of the software. 

The performance bottle neck in the poor_perf are the following, 

1. It reads the csv file twice increasing processing time.
2. Two for loops to read the entire file add extra processing time. 
3. Large number of if statements for the string comparison are perforamnce bottle neck in the poor_perf module. 

The above issues are ironed out in the good_perf module and it is evident in the perforamnce analysis. 




