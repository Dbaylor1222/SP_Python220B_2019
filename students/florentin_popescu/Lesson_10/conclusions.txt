Conclusions - Lesson 10

1) CSV files for products, customers and rentals with same number of records have been generated and inserted into MongoDB at the same time. Following files insertion, MongoDB collections have been dropped off. The insertion-drop step has been repeated many times for file sizes from 10 to 400K. At each repetition the insertion and drop have been timed and recorded into timing.txt.

2) Insertion time is found to be direct proportional with the number of records in each file. This indicate that file records are inserted one by one at roughly equal insertion time per record (despite the fact that 'insert_many()' was used). Each record's insertion time depends on record's complexity; the more information the individual record contains, the more time it takes to insert. For example, customer file has same number of records as product and rental files, but it has the richest schema among all so it takes longer to insert (at 100K records per file it took around 4.47s for inserting customer file vs. 3.62s for products and 3.22s for rentals). Moreover, it appears that once the number of records exceeds 0.25M the dependence starts deviating slightly from linearity. Additionally, the time for showing products/rentals also grows linearly with the size of MongoDB collection indicating that records from collections are rendered mainly one by one to the user for viewing.    

3) Collections removal time exhibits a very weak, almost insignificant, dependence on the number of records they hold. This is an indication that dropping collections from MongoDB is a highly efficient process and records are removed in big chunks rather than individually. The correlation and regression analysis show no clear relationship between collection’s size and its removal time. A slight upside trend in the regression line, along with a positive and weak correlation (40%), is observed suggesting a direct proportionality relationship, but the presence of high fluctuations in drop time prevents reaching a definite conclusion.