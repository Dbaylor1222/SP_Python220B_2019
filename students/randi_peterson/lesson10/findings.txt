I did two runs, the basic test with <10 entries per file in each of rentals, products, and customers. Then I did a "long" test with hundreds of entries per file to give a good contrast.

For the "13 records" run (5 customers, 4 prodcuts, 8 rentals of 4 products): 
	import_data: 0.08003425598144531 seconds
	show_available_products: 0.025017499923706055 seconds
	show_rentals: 0.020014524459838867 seconds

For the "1385" records run (500 customers, 200 products, 885 rentals of 4 products):
	import_data: 0.3162205219268799 seconds 
	show_available_products: 0.010007143020629883 seconds
	show_rentals: 0.5553939342498779 seconds

106x as many records
100x as many customers
40x as many products
110x as many rentals

import_data was 3.95x slower
show_available_products was almost 2x faster
show_rentals was 27.7x slower

The code will get slower with more entries. 
import_data did not get as much slowed down as I expected, but improvements here would still be helpful.
show_available_products speeding up did not make sense to me. On a following run, the smaller data set was faster, but only by about 0.001 seconds.
so this part of the code does not appear to be slowing it down with increasing products
show_rentals got very slow with a larger data set, this has the most opportunity for improvement

Further studies should be done with more intelligently chosen file sizes to get a good sense of the trends for comparison.
