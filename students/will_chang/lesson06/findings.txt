1. Small error in poor_perf.py on line:
       (Original) year_count["2017"] += 1
       (New) year_count["2018"] += 1

2. Tried condensing following code into a list comprehension. It only extended the
   execution time so a different approach was taken.
    (original - time: 4.963940858840942 seconds)
    for row in reader:
       lrow = list(row)
           if lrow[5] > '00/00/2012':
               new_ones.append((lrow[5], lrow[0])
    
    (list comprehension - time: 5.000530957165659 seconds)
    new_ones = [(list(row)[5], list(row)[0]) for row in reader if list(row)[5] > '00/00/2012']
    
3. Implemented a direct check to see if the date falls within 2013-2018. This approach
   reduces the execution time by about 25%.

   for row in reader:
       year = list(row)[5][6:]
           if year in year_count:
               year_count[year] += 1
               
4. There's no reason to open and read through the csv twice (once for year_count and
   once to count "ao". Combined the count sequence into the same block of code. This
   in conjunction with the improvement from step 3 reduces the execution time by
   about 60%.
   
   for row in reader:
       year = list(row)[5][6:]
           if year in year_count:
               year_count[year] += 1
           if "ao" in row[6]:
               found += 1
               
5. Just realized that row does not need to be converted to a list. Gained a slight
   improvement in performance by reducing this step. With steps 3-5, execution time
   was reduced by about 62%.